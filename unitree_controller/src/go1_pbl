#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import rospy
import os
import json
import numpy as np
import math
from math import pi
import random
import time
import csv
import pandas as pd
import matplotlib.pyplot as plt
import sys
sys.path.append(os.path.dirname(os.path.abspath(os.path.dirname(__file__))))
from collections import deque
from std_msgs.msg import Float32MultiArray,Float32
from tf.transformations import euler_from_quaternion, quaternion_from_euler
from sensor_msgs.msg import JointState
from geometry_msgs.msg import Twist, Point, Pose
from std_srvs.srv import Empty
from nav_msgs.msg import Odometry
from gazebo_msgs.msg import ModelState,ModelStates
from gazebo_msgs.srv import SetModelState
from datetime import datetime
#強化学習ライブラリ
import pfrl
import torch
import torch.nn as nn
import torch.optim as optim
from pfrl import experiments, replay_buffers, utils
from pfrl.agents import PPO

from respawnGoal import Respawn

class PPOModel(nn.Module):
    def __init__(self, obs_size, n_actions):
        super(PPOModel, self).__init__()
        # ポリシーネットワーク（行動を選択）
        self.policy = nn.Sequential(
            nn.Linear(obs_size, 128),
            nn.ReLU(),
            nn.Linear(128, 128),
            nn.ReLU(),
            nn.Linear(128, 128),
            nn.ReLU(),
            nn.Linear(128, n_actions),
            pfrl.policies.GaussianHeadWithStateIndependentCovariance(
                action_size=n_actions,
                var_type="diagonal",
                var_func=lambda x: torch.exp(2 * x),  # Parameterize log std
                var_param_init=0,  # log std = 0 => std = 1
            ),
        )

        # 価値関数ネットワーク（状態の価値を評価）
        self.value = nn.Sequential(
            nn.Linear(obs_size, 128),
            nn.ReLU(),
            nn.Linear(128, 128),
            nn.ReLU(),
            nn.Linear(128, 128),
            nn.ReLU(),
            nn.Linear(128, 1),
        )

    def forward(self, x):
        return self.policy(x), self.value(x)

class ReinforceAgent():
    def __init__(self):

        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.goal_x = 0
        self.goal_y = 0
        self.heading = 0
        self.collision = False
        self.sub_odom = rospy.Subscriber('gazebo/model_states', ModelStates, self.getOdometry,queue_size = 1)
        self.reset_proxy = rospy.ServiceProxy('gazebo/reset_simulation', Empty)
        self.reset_proxy2 = rospy.ServiceProxy('gazebo/reset_world', Empty)
        self.unpause_proxy = rospy.ServiceProxy('gazebo/unpause_physics', Empty)
        self.pause_proxy = rospy.ServiceProxy('gazebo/pause_physics', Empty)

        self.net = PPOModel(obs_size=2, n_actions=2).to(device=self.device)
        optimizer = optim.Adam(self.net.parameters(), lr=3e-4)
        self.model = PPO(
                model=self.net,
                optimizer=optimizer,
                gpu=(0 if str(self.device)=="cuda" else -1),  # -1でGPUを使わない設定
                gamma=0.99,  # 割引率
                lambd=0.95,  # GAE (Generalized Advantage Estimation) のパラメータ
                entropy_coef=0.01,  # エントロピー正則化の係数
                update_interval=2048,  # エージェントがネットワークを更新する間隔
                minibatch_size=64,  # ミニバッチのサイズ
                epochs=10,  # ポリシーを更新するエポック数
                clip_eps=0.2,  # PPOでクリッピングを行う範囲
                standardize_advantages=True,  # アドバンテージを標準化
            )

    def getGoalDistace(self):
        goal_distance = round(math.hypot(self.goal_x - self.position.x, self.goal_y - self.position.y), 2)
        return goal_distance

    def getOdometry(self, odom):
        self.position = odom.pose[2].position
        orientation = odom.pose[2].orientation
        orientation_list = [orientation.x, orientation.y, orientation.z, orientation.w]
        self.roll, _, yaw = euler_from_quaternion(orientation_list)

        self.goal_angle = math.atan2(self.goal_y - self.position.y, self.goal_x - self.position.x)

        heading = self.goal_angle - yaw
        if heading > pi:
            heading -= 2 * pi

        elif heading < -pi:
            heading += 2 * pi

        self.heading = round(heading, 2)

        if abs(self.roll) > 0.3:
            state_msg = ModelState()
            state_msg.model_name = 'go1_gazebo'
            state_msg.pose.position.x = 0.0
            state_msg.pose.position.y = 0.0
            state_msg.pose.position.z = 0.35
            q=quaternion_from_euler(0,0,0)
            state_msg.pose.orientation.x = q[0]
            state_msg.pose.orientation.y = q[1]
            state_msg.pose.orientation.z = q[2]
            state_msg.pose.orientation.w = q[3]
            rospy.wait_for_service('/gazebo/set_model_state')
            set_state = rospy.ServiceProxy('/gazebo/set_model_state', SetModelState)
            set_state( state_msg )

    def Reward(self,goal,collision,action):
        score = 0
        goal_distance = self.getGoalDistace()
        score = -(goal_distance**2)
        if goal:
            score += 100
        if collision:
            score -= 1000

        goal_vector = [math.cos(self.goal_angle)*goal_distance, math.sin(self.goal_angle)*goal_distance]
        move_vector = [action[0],action[1]]

        dot_product = np.dot(goal_vector, move_vector)

        # ベクトルのノルムの計算
        goal_norm = np.linalg.norm(goal_vector)
        move_norm = np.linalg.norm(move_vector)

        # コサイン類似度の計算
        if goal_norm != 0 and move_norm != 0:
            cos_similarity = dot_product / (move_vector[0]**2+move_vector[1]**2)**0.5
        else:
            cos_similarity = 0

        score+= cos_similarity*10

        return score

    def step(self, action, goal_distance):
        self.pub_cmd_vel = rospy.Publisher('cmd_vel', Twist, queue_size=5)
        rate = rospy.Rate(50)
        vel_cmd = Twist()
        vel_cmd.linear.x = action[0] #直進方向 m/s
        vel_cmd.linear.y = action[1]
        # vel_cmd.angular.z = action[2] #回転方向 rad/s
        self.pub_cmd_vel.publish(vel_cmd) #実行
        rate.sleep()
        done = False
        goal = False
        collision = False
        if goal_distance <0.2: ##0.2
            done = True
            goal = True
            vel_cmd = Twist()
            vel_cmd.linear.x = 0.0 #直進方向m/s
            vel_cmd.linear.y = 0.0
            vel_cmd.angular.z =0.0  #回転方向 rad/s
            self.pub_cmd_vel.publish(vel_cmd) #実行
            time.sleep(2)
            self.goal_x, self.goal_y = self.respawn_goal.getPosition(True,delete=True)

        if abs(self.position.x)>1.2 or abs(self.position.y)>1.2:
            done = True
            collision = True
        if abs(self.roll)>0.3:
            done = True
            collision = True
        if self.collision:
            done = True
            collision = True


        reward = self.Reward(goal,collision,action)

        return reward, done, goal, collision

    def train(self,Set=0):

        # self.model.load(os.path.dirname(os.path.realpath(__file__))+'/'+'30000')
        self.respawn_goal = Respawn()
        self.goal_x, self.goal_y = self.respawn_goal.getPosition()
        goal_distance = self.getGoalDistace()
        command = [goal_distance,self.heading]
        state = np.asarray(command)
        state = torch.from_numpy(state).float().to(device=self.device)
        start_time = time.time()
        goals = 0
        collisions = 0
        score = 0
        self.create_csv=True
        for t in range(1,200000+1):
            action = self.model.act(state)
            action = np.clip(action, -1.0, 1.0)
            goal_distance = self.getGoalDistace()

            reward, done, goal, collision= self.step(action,goal_distance)

            if goal:
                goals += 1

            if collision:
                collisions += 1

            score += reward

            if collision:

                while True:
                    state_msg = ModelState()
                    state_msg.model_name = 'go1_gazebo'
                    state_msg.pose.position.x = 0.0
                    state_msg.pose.position.y = 0.0
                    state_msg.pose.position.z = 0.35
                    q=quaternion_from_euler(0,0,0)
                    state_msg.pose.orientation.x = q[0]
                    state_msg.pose.orientation.y = q[1]
                    state_msg.pose.orientation.z = q[2]
                    state_msg.pose.orientation.w = q[3]
                    rospy.wait_for_service('/gazebo/set_model_state')
                    set_state = rospy.ServiceProxy('/gazebo/set_model_state', SetModelState)
                    set_state( state_msg )

                    reset = True
                    wait_time = time.time()
                    while True:
                        if abs(self.roll)>0.1:
                            break
                        vel_cmd = Twist()
                        vel_cmd.linear.x = 0.0 #直進方向m/s
                        vel_cmd.linear.y = 0.0
                        vel_cmd.angular.z =0.0  #回転方向 rad/s
                        self.pub_cmd_vel.publish(vel_cmd) #実行
                        now=time.time()
                        if now-wait_time>=2:
                            reset=False
                            break
                    if reset == False:
                        break

            command = [goal_distance,self.heading]
            next_state = np.asarray(command)
            next_state = torch.from_numpy(next_state).float().to(device=self.device)

            self.model.observe(next_state,reward,done,reset=False)

            state = next_state

            if t % 1000 == 0:
                m, s = divmod(int(time.time() - start_time), 60)
                h, m = divmod(m, 60)

                rospy.loginfo('Step: %d goal: %d collision: %d reward: %.2f time: %d:%02d:%02d', t, goals, collisions, score, h, m, s)
                time_str = f"{h}:{m:02d}:{s:02d}"
                info = [t, goals, collisions, score, time_str]
                self.write_to_csv(os.path.dirname(os.path.realpath(__file__))+'/output.csv', info)
                goals = 0
                collisions = 0
                score = 0
                self.model.save(os.path.dirname(os.path.realpath(__file__))+'/'+str(t))
        print('train finish')
        self.plot()

    def play(self,Set=0):
        self.model.load(os.path.dirname(os.path.realpath(__file__))+'/'+'10000')
        self.respawn_goal = Respawn()
        self.goal_x, self.goal_y = self.respawn_goal.getPosition()
        goal_distance = self.getGoalDistace()
        command = [goal_distance,self.heading]
        state = np.asarray(command)
        state = torch.from_numpy(state).float().to(device=self.device)
        start_time = time.time()
        goals = 0
        collisions = 0
        score = 0
        self.create_csv=True

        with self.model.eval_mode():
            a = time.time()
            for t in range(1,10000+1):
                b = time.time()
                # print(b-a)
                a = time.time()
                self.prev_position_x = self.position.x
                self.prev_position_y = self.position.y
                action = self.model.act(state)
                action = np.clip(action, -0.3, 0.3)
                goal_distance = self.getGoalDistace()
                reward, done, goal, collision= self.step(action,goal_distance)


                if goal:
                    goals += 1

                if collision:
                    collisions += 1

                score += reward

                if collision:
                    vel_cmd = Twist()
                    vel_cmd.linear.x = 0.0 #直進方向m/s
                    vel_cmd.linear.y = 0.0
                    vel_cmd.angular.z =0.0  #回転方向 rad/s
                    self.pub_cmd_vel.publish(vel_cmd) #実行

                    state_msg = ModelState()
                    state_msg.model_name = 'go1_gazebo'
                    state_msg.pose.position.x = 0.0
                    state_msg.pose.position.y = 0.0
                    state_msg.pose.position.z = 0.35
                    q=quaternion_from_euler(0,0,0)
                    state_msg.pose.orientation.x = q[0]
                    state_msg.pose.orientation.y = q[1]
                    state_msg.pose.orientation.z = q[2]
                    state_msg.pose.orientation.w = q[3]
                    rospy.wait_for_service('/gazebo/set_model_state')
                    set_state = rospy.ServiceProxy('/gazebo/set_model_state', SetModelState)
                    set_state( state_msg )

                command = [goal_distance,self.heading]
                next_state = np.asarray(command)
                next_state = torch.from_numpy(next_state).float().to(device=self.device)
                state = next_state

                if t % 1000 == 0:
                    m, s = divmod(int(time.time() - start_time), 60)
                    h, m = divmod(m, 60)

                    rospy.loginfo('Step: %d goal: %d collision: %d reward: %.2f time: %d:%02d:%02d', t, goals, collisions, score, h, m, s)
                    time_str = f"{h}:{m:02d}:{s:02d}"
                    info = [t, goals, collisions, score, time_str]
                    self.write_to_csv(os.path.dirname(os.path.realpath(__file__))+'/output.csv', info)
                    goals = 0
                    collisions = 0
                    score = 0
            print('play finish')
            self.plot()

    def plot(self):
        data = pd.read_csv(os.path.dirname(os.path.realpath(__file__))+'/output.csv')

        t = np.array(data['Step'])
        score = np.array(data['reward'])

        plt.plot(t, score, label='Cumulative reward over 1000steps', marker='o')
        plt.title('Transition of Rewards')
        plt.xlabel('Step')
        plt.ylabel('Reward')
        plt.legend()
        plt.savefig(os.path.dirname(os.path.realpath(__file__)) + '/reward_transition.png')
        plt.show()

    def write_to_csv(self, filename, data):
        if self.create_csv:
            with open(filename, mode='w', newline='') as file:
                writer = csv.writer(file)
                label = ['Step', 'goal', 'collision', 'reward', 'time']
                writer.writerow(label)
            self.create_csv = False
        with open(filename, mode='a', newline='') as file:
            writer = csv.writer(file)
            writer.writerow(data)

if __name__ == '__main__':
    rospy.init_node('go1_pbl')
    agent = ReinforceAgent()
    agent.train()
    # agent.play()
    # agent.plot()
